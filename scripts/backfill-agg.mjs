#!/usr/bin/env node
import { drizzle } from "drizzle-orm/vercel-postgres";
import { createPool } from "@vercel/postgres";
import { sql } from "drizzle-orm";

const pool = createPool({
  connectionString: process.env.DATABASE_URL || process.env.POSTGRES_URL
});

const db = drizzle(pool);

// è§£æå‘½ä»¤è¡Œå‚æ•°
function parseArgs() {
  const args = process.argv.slice(2);
  const options = {
    from: null,
    to: null,
    dryRun: false,
    granularity: "both", // hourly, daily, both
    help: false
  };

  for (let i = 0; i < args.length; i++) {
    const arg = args[i];
    switch (arg) {
      case "--from":
        options.from = args[++i];
        break;
      case "--to":
        options.to = args[++i];
        break;
      case "--dry-run":
        options.dryRun = true;
        break;
      case "--granularity":
        options.granularity = args[++i];
        break;
      case "--help":
      case "-h":
        options.help = true;
        break;
    }
  }

  return options;
}

function printHelp() {
  console.log(`
ä½¿ç”¨æ–¹æ³•: npm run db:backfill-agg [é€‰é¡¹]

é€‰é¡¹:
  --from <date>          èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºæœ€æ—©çš„è®°å½•æ—¥æœŸ
  --to <date>            ç»“æŸæ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºä»Šå¤©
  --granularity <type>   èšåˆç²’åº¦: hourly, daily, both (é»˜è®¤: both)
  --dry-run              ä»…æ˜¾ç¤ºå°†è¦æ‰§è¡Œçš„æ“ä½œï¼Œä¸å®é™…å†™å…¥æ•°æ®åº“
  --help, -h             æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯

ç¤ºä¾‹:
  npm run db:backfill-agg --from 2024-01-01 --to 2024-01-31
  npm run db:backfill-agg --granularity daily --dry-run
  npm run db:backfill-agg --from 2024-01-01
`);
}

// è·å–æ—¶é—´èŒƒå›´
async function getDateRange(from, to) {
  let startDate = from;
  let endDate = to;

  // å¦‚æœæ²¡æœ‰æŒ‡å®šèµ·å§‹æ—¥æœŸï¼ŒæŸ¥è¯¢æœ€æ—©çš„è®°å½•
  if (!startDate) {
    const result = await db.execute(sql`
      SELECT DATE(MIN(occurred_at)) as min_date
      FROM usage_records
    `);
    startDate = result.rows[0]?.min_date;
    if (!startDate) {
      console.log("âš ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰è®°å½•");
      return null;
    }
  }

  // å¦‚æœæ²¡æœ‰æŒ‡å®šç»“æŸæ—¥æœŸï¼Œä½¿ç”¨ä»Šå¤©
  if (!endDate) {
    endDate = new Date().toISOString().split("T")[0];
  }

  return { startDate, endDate };
}

// å›å¡«å°æ—¶èšåˆæ•°æ®
async function backfillHourly(startDate, endDate, dryRun) {
  console.log(`\nğŸ“Š å›å¡«å°æ—¶èšåˆæ•°æ®: ${startDate} è‡³ ${endDate}`);

  if (dryRun) {
    console.log("ğŸ” [DRY RUN] é¢„è§ˆå°†è¦æ‰§è¡Œçš„æ“ä½œ...");
    const preview = await db.execute(sql`
      SELECT
        DATE_TRUNC('hour', occurred_at) as bucket_start,
        route,
        model,
        COUNT(*) as record_count
      FROM usage_records
      WHERE occurred_at >= ${startDate}::date
        AND occurred_at < (${endDate}::date + INTERVAL '1 day')
      GROUP BY DATE_TRUNC('hour', occurred_at), route, model
      ORDER BY bucket_start DESC
      LIMIT 10
    `);

    console.log(`   å°†å¤„ç† ${preview.rowCount} ä¸ªå°æ—¶æ¡¶ï¼ˆæ˜¾ç¤ºå‰ 10 ä¸ªï¼‰:`);
    preview.rows.forEach(row => {
      console.log(`   - ${row.bucket_start} | ${row.route} | ${row.model} (${row.record_count} æ¡è®°å½•)`);
    });
    return;
  }

  const result = await db.execute(sql`
    INSERT INTO usage_hourly_agg (
      bucket_start,
      route,
      model,
      total_tokens,
      input_tokens,
      output_tokens,
      reasoning_tokens,
      cached_tokens,
      total_requests,
      success_count,
      failure_count,
      created_at,
      updated_at
    )
    SELECT
      DATE_TRUNC('hour', occurred_at) as bucket_start,
      route,
      model,
      SUM(total_tokens)::bigint as total_tokens,
      SUM(input_tokens)::bigint as input_tokens,
      SUM(output_tokens)::bigint as output_tokens,
      SUM(reasoning_tokens)::bigint as reasoning_tokens,
      SUM(cached_tokens)::bigint as cached_tokens,
      SUM(total_requests)::bigint as total_requests,
      SUM(success_count)::bigint as success_count,
      SUM(failure_count)::bigint as failure_count,
      NOW() as created_at,
      NOW() as updated_at
    FROM usage_records
    WHERE occurred_at >= ${startDate}::date
      AND occurred_at < (${endDate}::date + INTERVAL '1 day')
    GROUP BY DATE_TRUNC('hour', occurred_at), route, model
    ON CONFLICT (bucket_start, route, model)
    DO UPDATE SET
      total_tokens = EXCLUDED.total_tokens,
      input_tokens = EXCLUDED.input_tokens,
      output_tokens = EXCLUDED.output_tokens,
      reasoning_tokens = EXCLUDED.reasoning_tokens,
      cached_tokens = EXCLUDED.cached_tokens,
      total_requests = EXCLUDED.total_requests,
      success_count = EXCLUDED.success_count,
      failure_count = EXCLUDED.failure_count,
      updated_at = NOW()
  `);

  console.log(`âœ“ å°æ—¶èšåˆå®Œæˆï¼Œå¤„ç†äº† ${result.rowCount} ä¸ªå°æ—¶æ¡¶`);
}

// å›å¡«æ—¥èšåˆæ•°æ®
async function backfillDaily(startDate, endDate, dryRun) {
  console.log(`\nğŸ“Š å›å¡«æ—¥èšåˆæ•°æ®: ${startDate} è‡³ ${endDate}`);

  if (dryRun) {
    console.log("ğŸ” [DRY RUN] é¢„è§ˆå°†è¦æ‰§è¡Œçš„æ“ä½œ...");
    const preview = await db.execute(sql`
      SELECT
        DATE_TRUNC('day', occurred_at) as day_start,
        route,
        model,
        COUNT(*) as record_count
      FROM usage_records
      WHERE occurred_at >= ${startDate}::date
        AND occurred_at < (${endDate}::date + INTERVAL '1 day')
      GROUP BY DATE_TRUNC('day', occurred_at), route, model
      ORDER BY day_start DESC
      LIMIT 10
    `);

    console.log(`   å°†å¤„ç† ${preview.rowCount} ä¸ªæ—¥æœŸæ¡¶ï¼ˆæ˜¾ç¤ºå‰ 10 ä¸ªï¼‰:`);
    preview.rows.forEach(row => {
      console.log(`   - ${row.day_start} | ${row.route} | ${row.model} (${row.record_count} æ¡è®°å½•)`);
    });
    return;
  }

  const result = await db.execute(sql`
    INSERT INTO usage_daily_agg (
      day_start,
      route,
      model,
      total_tokens,
      input_tokens,
      output_tokens,
      reasoning_tokens,
      cached_tokens,
      total_requests,
      success_count,
      failure_count,
      created_at,
      updated_at
    )
    SELECT
      DATE_TRUNC('day', occurred_at) as day_start,
      route,
      model,
      SUM(total_tokens)::bigint as total_tokens,
      SUM(input_tokens)::bigint as input_tokens,
      SUM(output_tokens)::bigint as output_tokens,
      SUM(reasoning_tokens)::bigint as reasoning_tokens,
      SUM(cached_tokens)::bigint as cached_tokens,
      SUM(total_requests)::bigint as total_requests,
      SUM(success_count)::bigint as success_count,
      SUM(failure_count)::bigint as failure_count,
      NOW() as created_at,
      NOW() as updated_at
    FROM usage_records
    WHERE occurred_at >= ${startDate}::date
      AND occurred_at < (${endDate}::date + INTERVAL '1 day')
    GROUP BY DATE_TRUNC('day', occurred_at), route, model
    ON CONFLICT (day_start, route, model)
    DO UPDATE SET
      total_tokens = EXCLUDED.total_tokens,
      input_tokens = EXCLUDED.input_tokens,
      output_tokens = EXCLUDED.output_tokens,
      reasoning_tokens = EXCLUDED.reasoning_tokens,
      cached_tokens = EXCLUDED.cached_tokens,
      total_requests = EXCLUDED.total_requests,
      success_count = EXCLUDED.success_count,
      failure_count = EXCLUDED.failure_count,
      updated_at = NOW()
  `);

  console.log(`âœ“ æ—¥èšåˆå®Œæˆï¼Œå¤„ç†äº† ${result.rowCount} ä¸ªæ—¥æœŸæ¡¶`);
}

// ä¸»å‡½æ•°
async function main() {
  const options = parseArgs();

  if (options.help) {
    printHelp();
    process.exit(0);
  }

  console.log("ğŸš€ å¼€å§‹å›å¡«é¢„èšåˆæ•°æ®...");
  console.log(`   ç²’åº¦: ${options.granularity}`);
  console.log(`   æ¨¡å¼: ${options.dryRun ? "DRY RUNï¼ˆé¢„è§ˆï¼‰" : "å®é™…æ‰§è¡Œ"}`);

  try {
    // è·å–æ—¶é—´èŒƒå›´
    const dateRange = await getDateRange(options.from, options.to);
    if (!dateRange) {
      process.exit(0);
    }

    const { startDate, endDate } = dateRange;
    console.log(`   æ—¶é—´èŒƒå›´: ${startDate} è‡³ ${endDate}`);

    // æ‰§è¡Œå›å¡«
    if (options.granularity === "hourly" || options.granularity === "both") {
      await backfillHourly(startDate, endDate, options.dryRun);
    }

    if (options.granularity === "daily" || options.granularity === "both") {
      await backfillDaily(startDate, endDate, options.dryRun);
    }

    if (options.dryRun) {
      console.log("\nâœ“ DRY RUN å®Œæˆï¼Œæœªå®é™…ä¿®æ”¹æ•°æ®åº“");
    } else {
      console.log("\nâœ“ å›å¡«å®Œæˆï¼");
    }

    process.exit(0);
  } catch (error) {
    console.error("âŒ å›å¡«å¤±è´¥:", error);
    process.exit(1);
  }
}

main();
